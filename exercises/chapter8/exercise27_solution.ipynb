{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 27 Solution - Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Train an autoencoder (on a dataset composed of circles) and apply it to the task of anomaly detection (using a dataset composed of circles and squares)\n",
    "- Generate a normal and anomalous dataset (modify the data generation parameters if needed)\n",
    "- Train the autoencoder on only the normal dataset (modify the neural network and training parameters if needed)\n",
    "- Apply the autoencoder to the anomolous dataset and compare the reconstruction with reconstructions obtained with normal data\n",
    "- Improve the autoencoder through the denoising autoencoder extension (corrupt the training data and modify the loss function)\n",
    "\n",
    "### Learning goals\n",
    "- Understand the autoencoder architecture and its training\n",
    "- Familiarize yourself with the autoencoder implementation\n",
    "- Use an autoencoder for anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**data generation parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 128\n",
    "domainLength = 1\n",
    "numberOfCircles = 5\n",
    "radius = 0.1\n",
    "\n",
    "numberOfSamples = 20  #200\n",
    "numberOfAnomolousSamples = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**neural network parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 4\n",
    "numberOfFilters = 3\n",
    "convolutionalLayers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "lr = 5e-3\n",
    "batchSize = 64\n",
    "regularization = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**helper functions to generated normal and anomolous data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateNonOverlappingCirclesInDomain(N, domainLength, numberOfCircles, radius):\n",
    "    domain = np.ones((N, N))\n",
    "    x = np.linspace(0, domainLength, N)\n",
    "    y = np.linspace(0, domainLength, N)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "\n",
    "    for i in range(numberOfCircles):\n",
    "        overlap = True\n",
    "        while overlap == True:\n",
    "            xc = np.random.uniform(radius, domainLength - radius)\n",
    "            yc = np.random.uniform(radius, domainLength - radius)\n",
    "\n",
    "            mask = (x - xc) ** 2 + (y - yc) ** 2 < radius ** 2\n",
    "            if ~np.any(domain[mask] == -1):\n",
    "                overlap = False\n",
    "        domain[mask] = -1\n",
    "    return domain\n",
    "\n",
    "\n",
    "def generateAnomolousDataInDomain(N, domainLength, numberOfCircles, radius, numberOfSquares):\n",
    "    domain = np.ones((N, N))\n",
    "    x = np.linspace(0, domainLength, N)\n",
    "    y = np.linspace(0, domainLength, N)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "\n",
    "    for i in range(numberOfCircles):\n",
    "        overlap = True\n",
    "        while overlap == True:\n",
    "            xc = np.random.uniform(radius, domainLength - radius)\n",
    "            yc = np.random.uniform(radius, domainLength - radius)\n",
    "\n",
    "            if i >= numberOfSquares:\n",
    "                mask = (x - xc) ** 2 + (y - yc) ** 2 < radius ** 2\n",
    "            else:\n",
    "                mask = x > xc - radius\n",
    "                mask *= x < xc + radius\n",
    "                mask *= y > yc - radius\n",
    "                mask *= y < yc + radius\n",
    "            if ~np.any(domain[mask] == -1):\n",
    "                overlap = False\n",
    "        domain[mask] = -1\n",
    "    return domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**data generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"data\", exist_ok=True)    # Ensure the data directory exists\n",
    "\n",
    "normalSamples = torch.zeros((numberOfSamples, 1, N, N))\n",
    "for i in range(numberOfSamples):\n",
    "    normalSamples[i, 0] = torch.from_numpy(\n",
    "        generateNonOverlappingCirclesInDomain(N, domainLength, numberOfCircles, radius)).to(torch.float32)\n",
    "torch.save(normalSamples, \"data/normalData.pt\")\n",
    "\n",
    "sampleList = []\n",
    "anomalySamples = torch.zeros((numberOfSamples, 1, N, N))\n",
    "for j in range(0, numberOfCircles + 1):  # 0 is not anomalous\n",
    "    anomalySamples = torch.zeros((numberOfAnomolousSamples, 1, N, N))\n",
    "    for i in range(numberOfAnomolousSamples):\n",
    "        anomalySamples[i, 0] = torch.from_numpy(\n",
    "            generateAnomolousDataInDomain(N, domainLength, numberOfCircles, radius, j)).to(torch.float32)\n",
    "    torch.save(anomalySamples, \"data/anomalyData\" + str(j) + \".pt\")\n",
    "    sampleList.append(anomalySamples)\n",
    "\n",
    "fig, ax = plt.subplots(1, 6, figsize=(10, 3))\n",
    "for i in range(6):\n",
    "    ax[i].imshow(sampleList[i][0, 0], origin='lower', cmap='jet', interpolation='none', vmin=-1, vmax=1)\n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise functions (Exercise 27.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(sample, noise_level=0.1):\n",
    "    \"\"\"Add Gaussian noise to the input sample\"\"\"\n",
    "    noise = torch.randn_like(sample) * noise_level\n",
    "    noisy_sample = sample + noise\n",
    "    noisy_sample = torch.clamp(noisy_sample, -1, 1)\n",
    "    return noisy_sample\n",
    "\n",
    "def add_salt_pepper_noise(sample, salt_prob=0.1, pepper_prob=0.1):\n",
    "    \"\"\"Add salt and pepper noise to the input sample\"\"\"\n",
    "    noisy_sample = sample.clone()\n",
    "    noise = torch.rand_like(sample)\n",
    "    noisy_sample[noise < pepper_prob] = -1  # pepper noise (black pixels)\n",
    "    noisy_sample[noise > (1 - salt_prob)] = 1  # salt noise (white pixels)\n",
    "    return noisy_sample\n",
    "\n",
    "def add_masking_noise(sample, mask_prob=0.1):\n",
    "    \"\"\"Add masking noise to the input sample\"\"\"\n",
    "    noisy_sample = sample.clone()\n",
    "    mask = torch.rand_like(sample) < mask_prob\n",
    "    noisy_sample[mask] = -1\n",
    "    return noisy_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand_like(sample) < 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of noise functions\n",
    "sample = normalSamples[0:1]  # Take first sample for visualization\n",
    "\n",
    "# Apply different noise types\n",
    "gaussian_noisy = add_gaussian_noise(sample, noise_level=0.1)\n",
    "salt_pepper_noisy = add_salt_pepper_noise(sample, salt_prob=0.1, pepper_prob=0.1)\n",
    "masking_noisy = add_masking_noise(sample, mask_prob=0.2)\n",
    "\n",
    "\n",
    "# Create visualization\n",
    "fig, ax = plt.subplots(1, 4, figsize=(12, 3))\n",
    "\n",
    "ax[0].imshow(sample[0, 0], origin='lower', vmin=-1, vmax=1, cmap='jet', interpolation='none')\n",
    "ax[0].set_title('Original')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(gaussian_noisy[0, 0], origin='lower', vmin=-1, vmax=1, cmap='jet', interpolation='none')\n",
    "ax[1].set_title('Gaussian Noise')\n",
    "ax[1].axis('off')\n",
    "\n",
    "ax[2].imshow(salt_pepper_noisy[0, 0], origin='lower', vmin=-1, vmax=1, cmap='jet', interpolation='none')\n",
    "ax[2].set_title('Salt & Pepper Noise')\n",
    "ax[2].axis('off')\n",
    "\n",
    "ax[3].imshow(masking_noisy[0, 0], origin='lower', vmin=-1, vmax=1, cmap='jet', interpolation='none')\n",
    "ax[3].set_title('Masking Noise')\n",
    "ax[3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print noise statistics\n",
    "print(f\"Original - Min: {sample.min():.3f}, Max: {sample.max():.3f}, Mean: {sample.mean():.3f}\")\n",
    "print(f\"Gaussian - Min: {gaussian_noisy.min():.3f}, Max: {gaussian_noisy.max():.3f}, Mean: {gaussian_noisy.mean():.3f}\")\n",
    "print(f\"Salt & Pepper - Min: {salt_pepper_noisy.min():.3f}, Max: {salt_pepper_noisy.max():.3f}, Mean: {salt_pepper_noisy.mean():.3f}\")\n",
    "print(f\"Masking - Min: {masking_noisy.min():.3f}, Max: {masking_noisy.max():.3f}, Mean: {masking_noisy.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**load generated data into PyTorch dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class normalDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = torch.load(\"data/normalData.pt\", weights_only=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class anomalyDataset(Dataset):\n",
    "    def __init__(self, degree):\n",
    "        self.data = torch.load(\"data/anomalyData\" + str(degree) + \".pt\", weights_only=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder architecture definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(torch.nn.Module):\n",
    "    def __init__(self, depth, numberOfFilters, convolutionalLayers, bottleneckConvolutions=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.depth = depth\n",
    "        self.convolutionalLayers = convolutionalLayers\n",
    "        self.bottleneckConvolutions = bottleneckConvolutions\n",
    "\n",
    "        FilterSizes = np.linspace(-1, depth - 1, depth + 1, dtype=np.int32)\n",
    "        FilterSizes[1:] = 2 ** FilterSizes[1:] * numberOfFilters\n",
    "        FilterSizes[0] = 1\n",
    "\n",
    "        self.convDown = torch.nn.ModuleList()\n",
    "        self.batchNormDown = torch.nn.ModuleList()\n",
    "        self.activationDown = torch.nn.ModuleList()\n",
    "        self.downsample = torch.nn.ModuleList()\n",
    "\n",
    "        self.convBottleneck = torch.nn.ModuleList()\n",
    "        self.batchNormBottleneck = torch.nn.ModuleList()\n",
    "        self.activationBottleneck = torch.nn.ModuleList()\n",
    "\n",
    "        self.convUp = torch.nn.ModuleList()\n",
    "        self.batchNormUp = torch.nn.ModuleList()\n",
    "        self.activationUp = torch.nn.ModuleList()\n",
    "        self.upsample = torch.nn.ModuleList()\n",
    "\n",
    "        for i in range(depth):\n",
    "            # downsampling\n",
    "            for j in range(convolutionalLayers):\n",
    "                if j == 0:\n",
    "                    self.convDown.append(torch.nn.Conv2d(FilterSizes[i], FilterSizes[i + 1], kernel_size=3, stride=1,\n",
    "                                                         padding=1))  # ADD SECOND LAYER\n",
    "                else:\n",
    "                    self.convDown.append(\n",
    "                        torch.nn.Conv2d(FilterSizes[i + 1], FilterSizes[i + 1], kernel_size=3, stride=1,\n",
    "                                        padding=1))  # ADD SECOND LAYER\n",
    "                self.batchNormDown.append(torch.nn.BatchNorm2d(FilterSizes[i + 1]))\n",
    "                self.activationDown.append(torch.nn.PReLU(init=0.2))\n",
    "                self.downsample.append(torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "                # layers at bottleneck\n",
    "            for j in range(convolutionalLayers):\n",
    "                self.convBottleneck.append(\n",
    "                    torch.nn.Conv2d(FilterSizes[-1], FilterSizes[-1], kernel_size=3, stride=1, padding=1))\n",
    "                self.batchNormBottleneck.append(torch.nn.BatchNorm2d(FilterSizes[-1]))\n",
    "                self.activationBottleneck.append(torch.nn.PReLU(init=0.2))\n",
    "\n",
    "            # upsampling\n",
    "            for j in range(convolutionalLayers):\n",
    "                if j == 0:\n",
    "                    self.convUp.append(\n",
    "                        torch.nn.Conv2d(FilterSizes[-i - 1], FilterSizes[-i - 2], kernel_size=3, stride=1, padding=1))\n",
    "                else:\n",
    "                    self.convUp.append(\n",
    "                        torch.nn.Conv2d(FilterSizes[-i - 2], FilterSizes[-i - 2], kernel_size=3, stride=1, padding=1))\n",
    "                self.batchNormUp.append(torch.nn.BatchNorm2d(FilterSizes[-i - 2]))\n",
    "                self.activationUp.append(torch.nn.PReLU(init=0.2))\n",
    "            self.upsample.append(torch.nn.Upsample(scale_factor=2,\n",
    "                                                   mode='nearest'))  # nearest instead of bilinear as field is not continuous\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "\n",
    "        # downsampling        \n",
    "        for i in range(self.depth):\n",
    "            for j in range(self.convolutionalLayers):\n",
    "                y = self.activationDown[i * self.convolutionalLayers + j](\n",
    "                    self.batchNormDown[i * self.convolutionalLayers + j](\n",
    "                        self.convDown[i * self.convolutionalLayers + j](y)))\n",
    "            y = self.downsample[i](y)\n",
    "\n",
    "        # bottleneck\n",
    "        if self.bottleneckConvolutions == True:\n",
    "            for j in range(self.convolutionalLayers):\n",
    "                y = self.activationBottleneck[j](self.batchNormBottleneck[j](self.convBottleneck[j](y)))\n",
    "\n",
    "        # upsampling\n",
    "        for i in range(self.depth):\n",
    "            y = self.upsample[i](y)\n",
    "            for j in range(self.convolutionalLayers):\n",
    "                y = self.activationUp[i * self.convolutionalLayers + j](\n",
    "                    self.batchNormUp[i * self.convolutionalLayers + j](\n",
    "                        self.convUp[i * self.convolutionalLayers + j](y)))\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**data preparation including training/validation split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = normalDataset()\n",
    "datasetTraining, datasetValidation = torch.utils.data.dataset.random_split(dataset, [0.9, 0.1])\n",
    "dataloaderTraining = DataLoader(datasetTraining, batch_size=batchSize, shuffle=True)\n",
    "dataloaderValidation = DataLoader(datasetValidation, batch_size=10000, shuffle=False)  # all samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**neural network instantiation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoencoder(depth, numberOfFilters, convolutionalLayers, bottleneckConvolutions=True)\n",
    "summary(model, (1, 1, 128, 128))\n",
    "print(\"achieved reduction in bottleneck: {:.2f}\".format(\n",
    "    (numberOfFilters * (depth + 1) * (128 / 2 ** depth) ** 2) / 128 ** 2))\n",
    "model = model.to(device) # move model to gpu, summary only works on cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**optimizer and history**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=regularization)\n",
    "\n",
    "costHistoryTrain = np.zeros(epochs)\n",
    "costHistoryValidation = np.zeros(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "start0 = start\n",
    "bestCost = 1e10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "    for batch, sample in enumerate(dataloaderTraining):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Choose which noise to apply (uncomment one of the following lines)\n",
    "        # sample = add_gaussian_noise(sample, noise_level=0.1)\n",
    "        sample = add_salt_pepper_noise(sample, salt_prob=0.1, pepper_prob=0.1)\n",
    "        # sample = add_masking_noise(sample, mask_prob=0.1)\n",
    "\n",
    "\n",
    "        sample = sample.to(device)  # move sample to gpu\n",
    "\n",
    "        cost = torch.sum((model(sample) - sample) ** 2) / len(sample) / 128 ** 2\n",
    "        costHistoryTrain[epoch] += cost.detach().cpu()\n",
    "\n",
    "        cost.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        del sample\n",
    "    costHistoryTrain[epoch] /= (batch + 1)\n",
    "\n",
    "    model.eval()\n",
    "    sample = next(iter(dataloaderValidation))  # get one batch of validation data\n",
    "    with torch.no_grad():\n",
    "        sample = sample.to(device)\n",
    "        cost = torch.sum((model(sample) - sample) ** 2) / len(sample) / 128 ** 2\n",
    "        costHistoryValidation[epoch] = cost.detach().cpu()\n",
    "\n",
    "    if (epoch % 10 == 0):\n",
    "        elapsed_time = time.perf_counter() - start\n",
    "        string = \"Epoch: {}/{}\\t\\tCost (Train): {:.3E}\\t\\tCost (Validation): {:.3E}\\nEpoch time: {:2f}\"\n",
    "        print(string.format(epoch, epochs - 1, costHistoryTrain[epoch], costHistoryValidation[epoch], elapsed_time))\n",
    "        start = time.perf_counter()\n",
    "\n",
    "    # early stopping\n",
    "    if bestCost > costHistoryValidation[epoch]:\n",
    "        bestCost = costHistoryValidation[epoch]\n",
    "        torch.save(model.state_dict(), \"model\")\n",
    "        bestEpoch = epoch\n",
    "\n",
    "print(\"Total elapsed time: {:2f}\".format(time.perf_counter() - start0))\n",
    "print(\"best epoch: {:d}\".format(bestEpoch))\n",
    "model.load_state_dict(torch.load(\"model\", map_location=device, weights_only=False))  # early stopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "print(\"validation cost: {:.2e} training cost: {:.2e}\".format(np.min(costHistoryValidation), np.min(costHistoryTrain)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training history**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.grid()\n",
    "ax.plot(costHistoryTrain, 'k')\n",
    "ax.plot(costHistoryValidation, 'r')\n",
    "ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prediction of normal training sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(dataloaderTraining))\n",
    "sample = sample.to(device)  # move sample to gpu\n",
    "index = 0\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(sample[index, 0].detach().cpu(), origin='lower', vmin=0, vmax=1, cmap='jet', interpolation='none')\n",
    "ax[1].imshow(model(sample)[index, 0].detach().cpu(), origin='lower', vmin=0, vmax=1, cmap='jet', interpolation='none')\n",
    "ax[2].imshow((sample[index, 0].detach().cpu() - model(sample)[index, 0].detach().cpu()) ** 2, origin='lower', cmap='jet', interpolation='none')\n",
    "for i in range(3):\n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prediction of normal validation sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(dataloaderValidation))\n",
    "sample = sample.to(device)  # move sample to gpu\n",
    "index = 0\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(sample[index, 0].detach().cpu(), origin='lower', vmin=0, vmax=1, cmap='jet')\n",
    "ax[1].imshow(model(sample)[index, 0].detach().cpu(), origin='lower', vmin=0, vmax=1, cmap='jet')\n",
    "ax[2].imshow((sample[index, 0].detach().cpu() - model(sample)[index, 0].detach().cpu()) ** 2, origin='lower', cmap='jet')\n",
    "for i in range(3):\n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prediction of anomolous (previously unseen & out of distribution) data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change degree to choose number of squares\n",
    "anomaly_dataset = anomalyDataset(degree=1)\n",
    "print(len(anomaly_dataset))\n",
    "anomalyPrediction = model(anomaly_dataset.data.to(device))  # move anomaly dataset to gpu\n",
    "\n",
    "index = 0\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(anomaly_dataset[index, 0].detach().cpu(), origin='lower', vmin=0, vmax=1, cmap='jet')\n",
    "ax[1].imshow(anomalyPrediction[index, 0].detach().cpu(), origin='lower', vmin=0, vmax=1, cmap='jet')\n",
    "ax[2].imshow((anomaly_dataset[index, 0].detach().cpu() - anomalyPrediction[index, 0].detach().cpu()) ** 2, origin='lower',\n",
    "             vmin=0, vmax=1, cmap='jet')\n",
    "for i in range(3):\n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**compute reconstruction error for varying degree of anomaly (i.e., number of squares)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = np.zeros((6, len(anomaly_dataset.data)))\n",
    "for degree in range(6):\n",
    "    anomaly_dataset = anomalyDataset(degree)\n",
    "    anomalyPrediction = model(anomaly_dataset.data.to(device))  # move anomaly dataset to gpu\n",
    "    errors[degree] = torch.mean((anomaly_dataset.data - anomalyPrediction.detach().cpu()) ** 2, dim=(1, 2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**histogram of reconstruction errors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfBins = 100\n",
    "bins = np.histogram(errors, bins=numberOfBins)[1]\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(errors[0], bins=bins, color='r', alpha=0.7, label='no squares, 5 circles')\n",
    "ax.hist(errors[1], bins=bins, color='b', alpha=0.7, label='1 square, 4 circles')\n",
    "ax.hist(errors[2], bins=bins, color='gray', alpha=0.7, label='2 squares, 3 circles')\n",
    "ax.hist(errors[3], bins=bins, color='k', alpha=0.7, label='3 squares, 2 circles')\n",
    "ax.hist(errors[4], bins=bins, color='orange', alpha=0.7, label='4 squares, 1 circle')\n",
    "ax.hist(errors[5], bins=bins, color='magenta', alpha=0.7, label='5 squares, no circles')\n",
    "\n",
    "ax.set_xlabel(\"reconstruction error\")\n",
    "ax.set_ylabel(\"number of structures\")\n",
    "\n",
    "legend = ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aibook-generative",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
