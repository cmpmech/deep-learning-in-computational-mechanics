{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 28 Solution - Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for this exercise is based on the [blog post](https://avandekleut.github.io/vae/) by \n",
    "Alexander Van de Kleut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Train a variational autoencoder (on a dataset composed of circles, squares, triangles and crosses) and generate new samples\n",
    "\n",
    "### Learning goals\n",
    "- Understand the difference between an autoencoder and a variational autoencoder\n",
    "- Familiarize yourself with the implementational details of variational autoencoders\n",
    "- Use the latent space to deepen your understanding on how autoencoders operate (and how the Kullback-Leibler divergence changes the behavior in the latent space: change beta to experience the change in influence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**data generation parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 128\n",
    "domainLength = 1\n",
    "numberOfCircles = 5\n",
    "radius = 0.12\n",
    "min_max_side_length = (0.8, 0.8)\n",
    "numberOfSamples = 1280"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**neural network parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2  # 16, 32, 64, 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3  # 2e-3 = nice structure, 1e-3 = also not bad, 5e-3 = too high\n",
    "batch_size = 16\n",
    "epochs = 20  #100 to improve the results\n",
    "beta = 300  # figures in chapter generated with beta = 1000, 200, 0 (for 2D latent space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**helper functions to generate shapes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_circle(N, domainLength, x_center, y_center, diameter):\n",
    "    domain = np.ones((N, N))\n",
    "    radius = diameter / 2\n",
    "    x = np.linspace(0, domainLength, N)\n",
    "    y = np.linspace(0, domainLength, N)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    mask = (X - x_center) ** 2 + (Y - y_center) ** 2 <= radius ** 2\n",
    "    domain[mask] = -1\n",
    "    return domain\n",
    "\n",
    "\n",
    "def draw_square(N, domainLength, x_center, y_center, side_length):\n",
    "    domain = np.ones((N, N))\n",
    "    x = np.linspace(0, domainLength, N)\n",
    "    y = np.linspace(0, domainLength, N)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    mask = (np.abs(X - x_center) <= side_length / 2) & (np.abs(Y - y_center) <= side_length / 2)\n",
    "    domain[mask] = -1\n",
    "    return domain\n",
    "\n",
    "\n",
    "def draw_triangle(N, domainLength, x_center, y_center, height):\n",
    "    domain = np.ones((N, N))\n",
    "    x = np.linspace(0, domainLength, N)\n",
    "    y = np.linspace(0, domainLength, N)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "\n",
    "    # Height of the equilateral triangle\n",
    "    side_length = 2 / np.sqrt(3) * height\n",
    "\n",
    "    # Calculate vertices of the triangle\n",
    "    v_top = (x_center, y_center - 2 * height / 3)  # Top vertex\n",
    "    v_left = (x_center - side_length / 2, y_center + height / 3)  # Bottom left vertex\n",
    "    v_right = (x_center + side_length / 2, y_center + height / 3)  # Bottom right vertex\n",
    "\n",
    "    # Create mask for points inside the triangle using cross products\n",
    "    mask = (\n",
    "            ((X - v_top[0]) * (v_left[1] - v_top[1]) - (Y - v_top[1]) * (v_left[0] - v_top[0]) >= 0) &\n",
    "            ((X - v_left[0]) * (v_right[1] - v_left[1]) - (Y - v_left[1]) * (v_right[0] - v_left[0]) >= 0) &\n",
    "            ((X - v_right[0]) * (v_top[1] - v_right[1]) - (Y - v_right[1]) * (v_top[0] - v_right[0]) >= 0)\n",
    "    )\n",
    "\n",
    "    domain[mask] = -1\n",
    "    return domain\n",
    "\n",
    "\n",
    "def draw_cross(N, domainLength, x_center, y_center, arm_length):\n",
    "    domain = np.ones((N, N))\n",
    "    x = np.linspace(0, domainLength, N)\n",
    "    y = np.linspace(0, domainLength, N)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "\n",
    "    arm_width = arm_length / 3\n",
    "\n",
    "    # Create masks for the vertical and horizontal bars of the cross\n",
    "    vertical_bar = (np.abs(X - x_center) <= arm_width / 2) & (np.abs(Y - y_center) <= arm_length / 2)\n",
    "    horizontal_bar = (np.abs(X - x_center) <= arm_length / 2) & (np.abs(Y - y_center) <= arm_width / 2)\n",
    "\n",
    "    # Combine the masks to form the cross\n",
    "    mask = vertical_bar | horizontal_bar\n",
    "    domain[mask] = -1\n",
    "    return domain\n",
    "\n",
    "\n",
    "# Function to generate either a circle, a square or a triangle\n",
    "def generate_shapes(N, domainLength, min_max_side_length, shape=None):\n",
    "    if shape is None:\n",
    "        shape = np.random.randint(0, 4)  # Random shape type between 0 and 3\n",
    "\n",
    "    if shape == 0:  # Circle\n",
    "        diameter = np.random.uniform(*min_max_side_length)\n",
    "        x_center = np.random.uniform(diameter / 2, domainLength - diameter / 2)\n",
    "        y_center = np.random.uniform(diameter / 2, domainLength - diameter / 2)\n",
    "        domain = draw_circle(N, domainLength, x_center, y_center, diameter)\n",
    "\n",
    "    elif shape == 1:  # Square\n",
    "        side_length = np.random.uniform(*min_max_side_length)\n",
    "        x_center = np.random.uniform(side_length / 2, domainLength - side_length / 2)\n",
    "        y_center = np.random.uniform(side_length / 2, domainLength - side_length / 2)\n",
    "        domain = draw_square(N, domainLength, x_center, y_center, side_length)\n",
    "\n",
    "    elif shape == 2:  # Triangle\n",
    "        height = np.random.uniform(*min_max_side_length)\n",
    "        side_length = 2 / np.sqrt(3) * height\n",
    "        x_center = np.random.uniform(side_length / 2, domainLength - side_length / 2)\n",
    "        y_center = np.random.uniform(side_length / 2, domainLength - side_length / 2)\n",
    "        domain = draw_triangle(N, domainLength, x_center, y_center, height)\n",
    "\n",
    "    else:  # Cross\n",
    "        arm_length = np.random.uniform(*min_max_side_length)\n",
    "        x_center = np.random.uniform(arm_length / 2, domainLength - arm_length / 2)\n",
    "        y_center = np.random.uniform(arm_length / 2, domainLength - arm_length / 2)\n",
    "        domain = draw_cross(N, domainLength, x_center, y_center, arm_length)\n",
    "\n",
    "    return domain, shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**data generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"data\", exist_ok=True)    # Ensure the data directory exists\n",
    "\n",
    "samples = np.zeros((numberOfSamples, 1, N, N))\n",
    "shapes = np.zeros((numberOfSamples, 1))\n",
    "for k in range(numberOfSamples):\n",
    "    domain, shape = generate_shapes(N, domainLength, min_max_side_length)\n",
    "    samples[k, 0] = domain\n",
    "    shapes[k] = shape\n",
    "torch.save(torch.from_numpy(samples).to(torch.float32), f\"data/shapeData{N}.pt\")\n",
    "torch.save(torch.from_numpy(shapes).to(torch.float32), f\"data/shapeLabels{N}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**visualization of each shape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['circle', 'square', 'triangle', 'cross']\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    sample = samples[shapes == i][1].squeeze()\n",
    "    ax.imshow(sample, cmap='jet')\n",
    "    ax.set_title(f'{labels[i]}')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational autoencoder architecture definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder with PReLU activation\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.PReLU(init=0.2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.PReLU(init=0.2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.PReLU(init=0.2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.PReLU(init=0.2),\n",
    "        )\n",
    "\n",
    "        # Flatten layer\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Linear layers for mean and log variance\n",
    "        self.fc_mu = nn.Linear(256 * 8 * 8, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * 8 * 8, latent_dim)\n",
    "\n",
    "        # Decoder input\n",
    "        self.decoder_input = nn.Linear(latent_dim, 256 * 8 * 8)\n",
    "\n",
    "        # Decoder with PReLU activation\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.PReLU(init=0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.PReLU(init=0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.PReLU(init=0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.flatten(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_logvar(x)\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.decoder_input(z)\n",
    "        x = x.view(-1, 256, 8, 8)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**load generated data into PyTorch dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class microstructureDataset(Dataset):\n",
    "    def __init__(self, data, labels=None, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        if self.labels is None:\n",
    "            return sample, 0\n",
    "        else:\n",
    "            label = self.labels[idx]\n",
    "            return sample, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, log_var, beta=1):\n",
    "    loss_fn = nn.MSELoss(reduction='sum')\n",
    "    recon_loss = loss_fn(recon_x, x)\n",
    "    kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return (recon_loss + beta * kl_loss) / x.size(0) / 128 ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**data preparation including training/validation split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(\"data/shapeData128.pt\", weights_only=False)\n",
    "data = data / 2 + 0.5  # does 0-1 normalization\n",
    "labels = torch.load('data/shapeLabels128.pt', weights_only=False)\n",
    "dataset = microstructureDataset(data, labels)\n",
    "datasetTraining, datasetValidation = torch.utils.data.dataset.random_split(dataset, [0.9, 0.1])\n",
    "dataloaderTraining = DataLoader(datasetTraining, batch_size=batch_size, shuffle=True)\n",
    "dataloaderValidation = DataLoader(datasetValidation, batch_size=batch_size, shuffle=False)  # all samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**visualize a random training sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(batch, labels) = next(iter(dataloaderTraining))\n",
    "img = batch[-1].squeeze()\n",
    "plt.imshow(img, cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**neural network instantiation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(latent_dim=latent_dim)\n",
    "print(summary(model, (1, 1, 128, 128)))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**optimizer and history**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "costHistoryTrain = np.zeros(epochs)\n",
    "costHistoryValidation = np.zeros(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "start0 = start\n",
    "bestCost = 1e10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch, (x, y) in enumerate(dataloaderTraining):\n",
    "        x = x.to(device)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        x_hat, mu, log_var = model(x)\n",
    "        loss = loss_function(x_hat, x, mu, log_var, beta)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    costHistoryTrain[epoch] = total_loss / (batch + 1)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "        for batch, (x, y) in enumerate(dataloaderValidation):\n",
    "            x = x.to(device)\n",
    "            x_hat, mu, log_var = model(x)\n",
    "            val_loss = loss_function(x_hat, x, mu, log_var, beta)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "        costHistoryValidation[epoch] = total_val_loss / (batch + 1)\n",
    "\n",
    "    if (epoch % 1 == 0):\n",
    "        elapsed_time = time.perf_counter() - start\n",
    "        string = \"Epoch: {}/{}\\t\\tCost function (Train): {:.3E}\\t\\tCost function (Validation): {:.3E}\\nElapsed time: {:2f}\"\n",
    "        print(string.format(epoch, epochs - 1, costHistoryTrain[epoch], costHistoryValidation[epoch], elapsed_time))\n",
    "        start = time.perf_counter()\n",
    "\n",
    "    # early stopping\n",
    "    if bestCost > costHistoryValidation[epoch]:\n",
    "        bestCost = costHistoryValidation[epoch]\n",
    "        torch.save(model.state_dict(), \"bestmodel.pt\")\n",
    "        bestEpoch = epoch\n",
    "\n",
    "print(\"Total elapsed time: {:2f}\".format(time.perf_counter() - start0))\n",
    "print(\"best epoch: {:d}\".format(bestEpoch))\n",
    "model.load_state_dict(\n",
    "    torch.load(\"bestmodel.pt\", map_location=device, weights_only=False))  # load best result from early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval().to(device=device)\n",
    "print(\"validation cost: {:.2e} training cost: {:.2e}\".format(np.min(costHistoryValidation), np.min(costHistoryTrain)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training history**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(costHistoryTrain, 'k')\n",
    "ax.plot(costHistoryValidation, 'r')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Cost function')\n",
    "ax.legend(['Training', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prediction of training sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_sample, train_labels) = next(iter(dataloaderTraining))\n",
    "train_pred = model(train_sample.to(device=device))[0]\n",
    "train_pred = train_pred.detach().cpu()\n",
    "index = 0\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(train_sample[index, 0], cmap='jet')\n",
    "ax[1].imshow(train_pred[index, 0], cmap='jet')\n",
    "ax[2].imshow((train_sample[index, 0] - train_pred[index, 0]) ** 2, cmap='jet')\n",
    "for i in range(3):\n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prediction of validation sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(val_sample, val_labels) = next(iter(dataloaderValidation))\n",
    "val_pred = model(val_sample.to(device=device))[0]\n",
    "val_pred = val_pred.detach().cpu()\n",
    "index = 0\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(val_sample[index, 0], cmap='jet')\n",
    "ax[1].imshow(val_pred[index, 0], cmap='jet')\n",
    "ax[2].imshow((val_sample[index, 0] - val_pred[index, 0]) ** 2, cmap='jet')\n",
    "for i in range(3):\n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**generate 16 random samples (not part of dataset)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(16, latent_dim).to(device=device)\n",
    "samples = model.decode(z)\n",
    "samples = samples.detach().cpu()\n",
    "fig, ax = plt.subplots(4, 4, figsize=(8, 8))\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        ax[i, j].imshow(samples[i * 4 + j, 0], cmap='jet')\n",
    "        ax[i, j].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**visualize latent space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if latent_dim == 2:\n",
    "    def plot_latent(vae, data):\n",
    "        markers = {0: 'o', 1: 's', 2: '^', 3: '+'}  # square, triangle, circle\n",
    "        colors = {0: 'k', 1: 'r', 2: 'dimgray', 3: 'darkgray'}  # red, green, blue\n",
    "        labels = {0: 'circle', 1: 'square', 2: 'triangle', 3: 'cross'}\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_axisbelow(True)\n",
    "\n",
    "        for i, (x, y) in enumerate(data):\n",
    "            mu, log_var = vae.encode(x.to(device))\n",
    "            z = vae.reparameterize(mu, log_var)\n",
    "            z = z.to('cpu').detach().numpy()\n",
    "            y = y.to('cpu').numpy().flatten()\n",
    "\n",
    "            for shape in np.unique(y):\n",
    "                mask = y == shape\n",
    "                plt.scatter(z[mask, 0], z[mask, 1], c=colors[shape], marker=markers[shape], label=f'{labels[shape]}')\n",
    "\n",
    "        square = matplotlib.patches.Rectangle((-1, -1), 2, 2, edgecolor='black', facecolor='none', linewidth=2.5)\n",
    "        ax.add_patch(square)\n",
    "\n",
    "        ax.set_xlabel(\"$h_1$\")\n",
    "        ax.set_ylabel(\"$h_2$\")\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        by_label = dict(zip(labels, handles))\n",
    "        legend = ax.legend(by_label.values(), by_label.keys())\n",
    "\n",
    "        plt.ylim(-3, 3)\n",
    "        plt.xlim(-3, 3)\n",
    "        legend.get_frame().set_linewidth(2.5)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    plot_latent(model, dataloaderTraining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**visualize samples in latent space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if latent_dim == 2:\n",
    "    def plot_reconstructed(autoencoder, r0=(-5, 5), r1=(-5, 5), n=12, resolution=128):\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        w = resolution\n",
    "        img = np.zeros((n * w, n * w))\n",
    "        for i, y in enumerate(np.linspace(*r1, n)):\n",
    "            for j, x in enumerate(np.linspace(*r0, n)):\n",
    "                z = torch.Tensor([[x, y]]).to(device)\n",
    "                x_hat = autoencoder.decode(z)\n",
    "                x_hat = x_hat.detach().cpu()\n",
    "                img[(n - 1 - i) * w:(n - 1 - i + 1) * w, j * w:(j + 1) * w] = x_hat\n",
    "        plt.imshow(img, extent=[*r0, *r1], cmap='jet')\n",
    "        plt.xticks([i for i in [-1, 0, 1]])  # adjust according to scale\n",
    "        plt.yticks([i for i in [-1, 0, 1]])  # adjust according to scale\n",
    "\n",
    "        plt.clim(0.0, 1.0)\n",
    "\n",
    "        ax.set_xlabel(\"$h_1$\")\n",
    "        ax.set_ylabel(\"$h_2$\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    lim = 1.0\n",
    "    plot_reconstructed(model, r0=(-lim, lim), r1=(-lim, lim), n=12, resolution=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Add noise to the inputs, just setting random values to "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
