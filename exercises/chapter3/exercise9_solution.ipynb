{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9 - Solution\n",
    "\n",
    "## Task\n",
    "Implement a multilayer Fully Connected Network from scratch using Numpy. Use the class structure provided below. Four member functions are to be implemented:\n",
    "- `forward(self, x)` for the forward prediction\n",
    "- `backward(self, y)` for the backpropagation in order to compute the gradients\n",
    "- `zero_grad(self)` to reset the gradients to zero (to be used before a second prediction/backpropagation)\n",
    "- `step(self, lr)` to update the weights and biases with gradient descent\n",
    "\n",
    "The most challenging task is to compute the gradients. Therefore split the implementation up into two steps:\n",
    "- implement the gradient computation for a sample size of one\n",
    "- implement the gradient computation for an arbitrary sample size (use the currentWeightGradients and currentBiasGradients to store the intermediate results)\n",
    "\n",
    "To check the results, you can verify the gradients by comparing them to PyTorch. A code copying the custom neural network to PyTorch, where the automatic differentiation is provided below.\n",
    "\n",
    "After a successful verification, use the neural network to learn a function. The training algorithm is provided below. Using the Adam optimizer implementation from exercise 2.3, the training can be improved. It is almost impossible to learn a sufficiently complex function with only gradient descent. \n",
    "\n",
    "Finally, compare the implementation to a PyTorch implementation.\n",
    "\n",
    "## Learning Goals\n",
    "- Gain a deeper understanding of each step in the fully connected network inference and backpropogation\n",
    "- Practice matching tensor dimensions of each input and output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints\n",
    "Backward: \n",
    "- (3.14) $$\\frac{\\partial z^{(l)}_{j}}{\\partial w^{(l)}_{jk}} = \\frac{\\partial}{\\partial w^{(l)}_{jk}} \\sum_h w^{(l)}_{jh} a^{(l-1)}_h + b^{(l)}_j = a^{(l-1)}_k$$ \n",
    "\n",
    "- (3.16) $$\\frac{\\partial C}{\\partial b^{(l)}_j} = \\frac{\\partial C}{\\partial z^{(l)}_{j}}\\frac{\\partial z^{(l)}_{j}}{\\partial b^{(l)}_j} = \\delta^{(l)}_j \\frac{\\partial z^{(l)}_{j}}{\\partial b^{(l)}_j} = \\delta^{(l)}_j$$\n",
    "\n",
    "- (3.18) $$\\delta^{(L)}_j = \\frac{\\partial C}{\\partial z^{(L)}_j} = \n",
    "\t\\frac{\\partial C}{\\partial a^{(L)}_j}\\frac{\\partial a^{(L)}_j}{\\partial z^{(L)}_j} =\n",
    "\t%\t\\frac{\\partial}{\\partial a^{(L)}_j} \\frac{1}{2}(y_j - a^{(L)}_j)^2\n",
    "\t\\frac{\\partial C}{\\partial a^{(L)}_j}\\sigma'(z^{(L)}_j) = \n",
    "\t-(\\tilde{y}_j - \\sigma(z^{(L)}_j))\\sigma'(z^{(L)}_j)$$\n",
    "\n",
    "- (3.23) \t$$\\delta^{(l)}_j = \\sum_k w^{(l+1)}_{kj}  \\delta^{(l+1)}_k \\sigma'(z^{(l)}_j)$$\n",
    "\n",
    "Start by implementing the backward propagation for only a single sample. Subsequently, extend the implementations for an arbitrary number of samples. To this end, recall the definitions provided in\n",
    "Equations \n",
    "\n",
    "(3.10) $$C = \\frac{1}{m_{\\mathcal{D}}}\\sum_{i=1}^{m_{\\mathcal{D}}}C_i$$\n",
    "\n",
    "and (3.11)    $$C_i = \\frac{1}{2}(\\tilde{y}_i - \\hat{y}_i)^2 = \\frac{1}{2}(\\tilde{y}_i - a_i^{(L)})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T11:30:45.871509056Z",
     "start_time": "2023-12-22T11:30:45.830133707Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**neural network class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T11:30:46.060521417Z",
     "start_time": "2023-12-22T11:30:46.055466127Z"
    }
   },
   "outputs": [],
   "source": [
    "class neuralNetwork:\n",
    "    def __init__(\n",
    "            self, layers, activation, activationGradient, xavierInitialization=True\n",
    "    ):\n",
    "        self.L = len(layers)  # number of layers\n",
    "        if xavierInitialization == True:\n",
    "            self.weights = [\n",
    "                torch.nn.init.xavier_uniform_(\n",
    "                    torch.zeros((layers[i], layers[i + 1])),\n",
    "                    gain=torch.nn.init.calculate_gain(\"sigmoid\"),\n",
    "                ).numpy()\n",
    "                for i in range(self.L - 1)\n",
    "            ]\n",
    "        else:\n",
    "            self.weights = [\n",
    "                np.random.rand(layers[i], layers[i + 1]) for i in range(self.L - 1)\n",
    "            ]\n",
    "        self.biases = [np.random.rand(1, layers[i + 1]) for i in range(self.L - 1)]\n",
    "\n",
    "        self.layerActivations = []\n",
    "\n",
    "        self.weightGradients = [\n",
    "            np.zeros((layers[i], layers[i + 1])) for i in range(self.L - 1)\n",
    "        ]\n",
    "        self.biasGradients = [np.zeros((1, layers[i + 1])) for i in range(self.L - 1)]\n",
    "\n",
    "        # helper variables to store gradients per sample\n",
    "        self.currentWeightGradients = [\n",
    "            np.zeros((layers[i], layers[i + 1])) for i in range(self.L - 1)\n",
    "        ]\n",
    "        self.currentBiasGradients = [\n",
    "            np.zeros((1, layers[i + 1])) for i in range(self.L - 1)\n",
    "        ]\n",
    "\n",
    "        self.activation = activation\n",
    "        self.activationGradient = activationGradient\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.layerActivations = []  # clear activations\n",
    "        a = x\n",
    "        self.layerActivations.append(a)  # activation is not applied to input\n",
    "        for i in range(self.L - 1):\n",
    "            z = a @ self.weights[i] + self.biases[i]\n",
    "            self.layerActivations.append(z)  # store activations\n",
    "            a = self.activation(z)\n",
    "        return a\n",
    "\n",
    "    def backward(self, y):\n",
    "        if len(self.layerActivations) > 0:  # check if forward pass has been performed\n",
    "            numberOfSamples = len(self.layerActivations[0])\n",
    "\n",
    "            # for a single sample\n",
    "            if numberOfSamples == 1:\n",
    "                deltaL = -(\n",
    "                        y - self.activation(self.layerActivations[self.L - 1])\n",
    "                ) * self.activationGradient(self.layerActivations[self.L - 1])\n",
    "                self.biasGradients[self.L - 2] = deltaL\n",
    "                for i in range(\n",
    "                        self.L - 2\n",
    "                ):  # indices of layer activation shifted due to stored input\n",
    "                    deltal = np.sum(\n",
    "                        self.weights[self.L - 2 - i]\n",
    "                        * self.biasGradients[self.L - 2 - i],\n",
    "                        1,\n",
    "                    ) * self.activationGradient(self.layerActivations[self.L - 2 - i])\n",
    "                    self.biasGradients[self.L - 3 - i] = deltal\n",
    "\n",
    "                self.weightGradients[0] = (\n",
    "                        np.transpose(self.layerActivations[0]) @ self.biasGradients[0]\n",
    "                )  # without activation for input\n",
    "                for i in range(1, self.L - 1):\n",
    "                    self.weightGradients[i] = (\n",
    "                            np.transpose(self.activation(self.layerActivations[i]))\n",
    "                            @ self.biasGradients[i]\n",
    "                    )\n",
    "\n",
    "            # for multiple samples using for loop over samples\n",
    "            elif numberOfSamples > 1:\n",
    "                for j in range(numberOfSamples):\n",
    "\n",
    "                    deltaL = -(\n",
    "                            y[j: j + 1]\n",
    "                            - self.activation(self.layerActivations[self.L - 1][j: j + 1])\n",
    "                    ) * self.activationGradient(\n",
    "                        self.layerActivations[self.L - 1][j: j + 1]\n",
    "                    )\n",
    "                    self.currentBiasGradients[self.L - 2] = (\n",
    "                            deltaL / numberOfSamples\n",
    "                    )  # division by number of samples, as we are taking the mean\n",
    "                    self.biasGradients[self.L - 2] += self.currentBiasGradients[\n",
    "                        self.L - 2\n",
    "                        ]\n",
    "                    for i in range(\n",
    "                            self.L - 2\n",
    "                    ):  # indices of layer activation shifted due to stored input\n",
    "                        deltal = np.sum(\n",
    "                            self.weights[self.L - 2 - i]\n",
    "                            * self.currentBiasGradients[self.L - 2 - i],\n",
    "                            1,\n",
    "                        ) * self.activationGradient(\n",
    "                            self.layerActivations[self.L - 2 - i][j: j + 1]\n",
    "                        )\n",
    "                        self.currentBiasGradients[self.L - 3 - i] = deltal\n",
    "                        self.biasGradients[self.L - 3 - i] += self.currentBiasGradients[\n",
    "                            self.L - 3 - i\n",
    "                            ]\n",
    "\n",
    "                    # without activation for input\n",
    "                    self.currentWeightGradients[0] = (\n",
    "                            np.transpose(self.layerActivations[0][j: j + 1])\n",
    "                            @ self.currentBiasGradients[0]\n",
    "                    )\n",
    "                    self.weightGradients[0] += self.currentWeightGradients[0]\n",
    "                    for i in range(1, self.L - 1):\n",
    "                        self.currentWeightGradients[i] = (\n",
    "                                np.transpose(\n",
    "                                    self.activation(self.layerActivations[i][j: j + 1])\n",
    "                                )\n",
    "                                @ self.currentBiasGradients[i]\n",
    "                        )\n",
    "                        self.weightGradients[i] += self.currentWeightGradients[i]\n",
    "\n",
    "        else:\n",
    "            print(\"backward propagation not possible\")\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.weightGradients = [\n",
    "            np.zeros((layers[i], layers[i + 1])) for i in range(self.L - 1)\n",
    "        ]\n",
    "        self.biasGradients = [np.zeros((1, layers[i + 1])) for i in range(self.L - 1)]\n",
    "\n",
    "    def step(self, lr):\n",
    "        for i in range(self.L - 1):\n",
    "            self.weights[i] -= lr * self.weightGradients[i]\n",
    "            self.biases[i] -= lr * self.biasGradients[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T11:30:46.361422445Z",
     "start_time": "2023-12-22T11:30:46.354778240Z"
    }
   },
   "outputs": [],
   "source": [
    "layers = [1, 4, 4, 1]\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "sigmoidGradient = lambda x: sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "model = neuralNetwork(layers, sigmoid, sigmoidGradient)\n",
    "\n",
    "# input data\n",
    "x = np.expand_dims(np.linspace(0, 1, 2), 1) + 0.2\n",
    "y = np.sin(4 * np.pi * x) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prediction, cost computation & gradient computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T11:30:46.624159965Z",
     "start_time": "2023-12-22T11:30:46.618394414Z"
    }
   },
   "outputs": [],
   "source": [
    "# prediction\n",
    "yPred = model.forward(x)\n",
    "\n",
    "# cost\n",
    "C = 0.5 * np.mean((yPred - y) ** 2)\n",
    "\n",
    "# gradient\n",
    "model.backward(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model definition and cloning of model parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T11:30:47.080256036Z",
     "start_time": "2023-12-22T11:30:47.073862834Z"
    }
   },
   "outputs": [],
   "source": [
    "class neuralNetworkTorch(torch.nn.Module):\n",
    "    def __init__(self, layers, activationFunction=torch.nn.Sigmoid()):\n",
    "        super().__init__()\n",
    "        modules = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            modules.append(torch.nn.Linear(layers[i], layers[i + 1]))\n",
    "            modules.append(activationFunction)\n",
    "\n",
    "        self.model = torch.nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "modelTorch = neuralNetworkTorch(layers)\n",
    "\n",
    "# copy parameters\n",
    "with torch.no_grad():\n",
    "    for i, param in enumerate(modelTorch.parameters()):\n",
    "        if i % 2 == 0:\n",
    "            param.data = torch.from_numpy(model.weights[i // 2]).to(torch.float64).t()\n",
    "        else:\n",
    "            param.data = torch.from_numpy(model.biases[i // 2]).to(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prediction, cost computation & gradient computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T11:30:47.378616583Z",
     "start_time": "2023-12-22T11:30:47.373348220Z"
    }
   },
   "outputs": [],
   "source": [
    "xTorch = torch.from_numpy(x).to(torch.float64)\n",
    "yPredTorch = modelTorch.forward(xTorch)\n",
    "\n",
    "CTorch = 0.5 * torch.mean((yPredTorch - torch.from_numpy(y).to(torch.float64)) ** 2)\n",
    "CTorch.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**gradient comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T11:30:47.691588005Z",
     "start_time": "2023-12-22T11:30:47.681767329Z"
    }
   },
   "outputs": [],
   "source": [
    "layer = 0\n",
    "# weight gradients\n",
    "print(\"weight:\")\n",
    "print(np.transpose(model.weightGradients[layer]))\n",
    "print(list(modelTorch.parameters())[2 * layer].grad)\n",
    "\n",
    "# bias gradients\n",
    "print(\"bias:\")\n",
    "print(model.biasGradients[layer])\n",
    "print(list(modelTorch.parameters())[2 * layer + 1].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 from Chapter 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T11:30:47.978647219Z",
     "start_time": "2023-12-22T11:30:47.973958649Z"
    }
   },
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None\n",
    "        self.n = None\n",
    "        self.t = 0\n",
    "\n",
    "    def updateParams(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m = [np.zeros_like(param) for param in params]\n",
    "        if self.n is None:\n",
    "            self.n = [np.zeros_like(param) for param in params]\n",
    "\n",
    "        self.t += 1  # exponent increases with epochs\n",
    "        updatedParams = []\n",
    "\n",
    "        for p, g, m, n in zip(params, grads, self.m, self.n):\n",
    "            m[:] = self.beta1 * m + (1 - self.beta1) * g\n",
    "            n[:] = self.beta2 * n + (1 - self.beta2) * (g ** 2)\n",
    "\n",
    "            mhat = m / (1 - self.beta1 ** self.t)\n",
    "            nhat = n / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            updatedParams.append(p - self.lr * mhat / (np.sqrt(nhat) + self.epsilon))\n",
    "\n",
    "        return updatedParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning with the custom neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**select optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T11:30:48.460758355Z",
     "start_time": "2023-12-22T11:30:48.455452319Z"
    }
   },
   "outputs": [],
   "source": [
    "selectOptimizer = \"gradientDescent\"\n",
    "# selectOptimizer = 'Adam'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T11:30:48.770635297Z",
     "start_time": "2023-12-22T11:30:48.762229551Z"
    }
   },
   "outputs": [],
   "source": [
    "layers = [1, 20, 20, 1]\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "sigmoidGradient = lambda x: sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "model = neuralNetwork(layers, sigmoid, sigmoidGradient)\n",
    "\n",
    "x = np.expand_dims(np.linspace(0, 1, 40), 1)  # training grid\n",
    "\n",
    "if selectOptimizer == \"gradientDescent\":\n",
    "    y = x ** 2\n",
    "    epochs = 10000\n",
    "    lr = 1e-1\n",
    "elif selectOptimizer == \"Adam\":\n",
    "    y = np.sin(2 * np.pi * x) ** 2  # a more difficult function\n",
    "    lr = 1e-2\n",
    "    optimizer = AdamOptimizer(lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T11:31:24.345017934Z",
     "start_time": "2023-12-22T11:30:49.072919393Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.zero_grad()\n",
    "    yPred = model.forward(x)\n",
    "    C = 0.5 * np.mean((yPred - y) ** 2)\n",
    "    model.backward(y)\n",
    "\n",
    "    if selectOptimizer == \"gradientDescent\":\n",
    "        model.step(lr)\n",
    "    elif selectOptimizer == \"Adam\":\n",
    "        updatedParameters = optimizer.updateParams(\n",
    "            model.weights + model.biases, model.weightGradients + model.biasGradients\n",
    "        )\n",
    "        model.weights = updatedParameters[: model.L - 1]\n",
    "        model.biases = updatedParameters[model.L - 1:]\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        elapsedTime = time.perf_counter() - start\n",
    "        string = \"Epoch: {}/{}\\t\\tCost = {:.2e}\\t\\tElapsed time = {:2f}\"\n",
    "        print(string.format(epoch, epochs, C, elapsedTime))\n",
    "        start = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**visualize the prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T11:31:24.439891943Z",
     "start_time": "2023-12-22T11:31:24.354472264Z"
    }
   },
   "outputs": [],
   "source": [
    "yPred = model.forward(x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.grid()\n",
    "ax.plot(x, y, \"k\", label=\"ground truth\")\n",
    "ax.plot(x, yPred, \"r--\", label=\"prediction\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T11:31:24.446982396Z",
     "start_time": "2023-12-22T11:31:24.441465519Z"
    }
   },
   "outputs": [],
   "source": [
    "modelTorch = neuralNetworkTorch(layers)\n",
    "xTorch = torch.from_numpy(x).to(torch.float32)\n",
    "yTorch = torch.from_numpy(y).to(torch.float32)\n",
    "if selectOptimizer == \"gradientDescent\":\n",
    "    optimizer = torch.optim.SGD(modelTorch.parameters(), lr)\n",
    "elif selectOptimizer == \"Adam\":\n",
    "    optimizer = torch.optim.Adam(modelTorch.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T11:31:29.386640956Z",
     "start_time": "2023-12-22T11:31:24.446398856Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    yPredTorch = modelTorch.forward(xTorch)\n",
    "    CTorch = 0.5 * torch.mean((yPredTorch - yTorch) ** 2)\n",
    "    CTorch.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        elapsedTime = time.perf_counter() - start\n",
    "        string = \"Epoch: {}/{}\\t\\tCost = {:.2e}\\t\\tElapsed time = {:2f}\"\n",
    "        print(string.format(epoch, epochs, CTorch.detach(), elapsedTime))\n",
    "        start = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**visualize the predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T11:31:29.479013182Z",
     "start_time": "2023-12-22T11:31:29.387061456Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.grid()\n",
    "ax.plot(x, y, \"k\", label=\"ground truth\")\n",
    "ax.plot(xTorch, yPredTorch.detach(), \"r--\", label=\"torch prediction\")\n",
    "ax.plot(xTorch, yPred, \"b--\", label=\"scratch prediction\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python . (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
