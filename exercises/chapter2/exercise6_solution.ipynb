{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 Solution - Adam\n",
    "### Task\n",
    "Implement the Adam optimizer. For help refer to algorithm 2. A class structure is provided. After the implementation, compare the Adam optimizer with standard gradient descent \n",
    "\n",
    "### Learning goals\n",
    "- Understand the Adam optimizer\n",
    "- Experience the difference between Adam and standard gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**define starting position and function to optimize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)  # Generate the data.\n",
    "x1 = 2.\n",
    "x2 = 2.\n",
    "params0 = [np.array([x1]), np.array([x2])]\n",
    "\n",
    "f = lambda x1, x2: 100 * (x2 - x1**2) ** 2 + (1 - x1) ** 2  # Rosenbrock function\n",
    "dfdx = lambda x1, x2: [\n",
    "    np.array(400 * (-x2 + x1**2) * x1 + 2 * (x1 - 1)),\n",
    "    np.array(200 * (x2 - x1**2)),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adam optimizer** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None  # list to store all first statistical moments\n",
    "        self.n = None  # list to store all second statistical moments\n",
    "        self.t = 0  # keeps track of how many epochs have been performed\n",
    "\n",
    "    def updateParams(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m = [np.zeros_like(param) for param in params]  # initializing list\n",
    "        if self.n is None:\n",
    "            self.n = [np.zeros_like(param) for param in params]  # initializing list\n",
    "\n",
    "        updatedParams = []\n",
    "\n",
    "        self.t += 1  # exponent increases with epochs\n",
    "\n",
    "        for p, g, m, n in zip(params, grads, self.m, self.n):\n",
    "            m[:] = self.beta1 * m + (1 - self.beta1) * g\n",
    "            n[:] = self.beta2 * n + (1 - self.beta2) * (g**2)\n",
    "\n",
    "            mhat = m / (1 - self.beta1**self.t)\n",
    "            nhat = n / (1 - self.beta2**self.t)\n",
    "\n",
    "            updatedP = p - self.lr * mhat / (np.sqrt(nhat) + self.epsilon)\n",
    "            updatedParams.append(updatedP)\n",
    "\n",
    "        return updatedParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**optimization with gradient descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3 \n",
    "epochs = 1000\n",
    "\n",
    "params = copy.deepcopy(params0)\n",
    "\n",
    "optimizationPathGD = np.zeros((2, epochs))\n",
    "for epoch in range(epochs):\n",
    "    cost = f(params[0], params[1]).item()\n",
    "    optimizationPathGD[0, epoch] = params[0]\n",
    "    optimizationPathGD[1, epoch] = params[1]\n",
    "    grad = dfdx(params[0], params[1])\n",
    "    \n",
    "    params[0] -= lr * grad[0]\n",
    "    params[1] -= lr * grad[1]\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        string = \"Epoch: {}/{}\\t\\tCost = {:.2e}\"\n",
    "        print(string.format(epoch, epochs, cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**optimization with Adam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-1\n",
    "epochs = 1000\n",
    "optimizer = AdamOptimizer(lr=lr)\n",
    "\n",
    "params = copy.deepcopy(params0)\n",
    "\n",
    "optimizationPathAdam = np.zeros((2, epochs))\n",
    "for epoch in range(epochs):\n",
    "    cost = f(params[0], params[1]).item()\n",
    "    optimizationPathAdam[0, epoch] = params[0]\n",
    "    optimizationPathAdam[1, epoch] = params[1]\n",
    "    grad = dfdx(params[0], params[1])\n",
    "\n",
    "    params = optimizer.updateParams(params, grad)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        string = \"Epoch: {}/{}\\t\\tCost = {:.2e}\"\n",
    "        print(string.format(epoch, epochs, cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**visualize the optimization path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_ = np.linspace(-1, 3, 200)\n",
    "x2_ = np.linspace(-1, 3, 200)\n",
    "x1_, x2_ = np.meshgrid(x1_, x2_, indexing=\"ij\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "cp = ax.pcolormesh(x1_, x2_, f(x1_, x2_), cmap=plt.cm.jet, norm=colors.LogNorm(), shading='auto')\n",
    "ax.plot(optimizationPathGD[0], optimizationPathGD[1], 'k', label=\"gradient descent\")\n",
    "ax.plot(optimizationPathGD[0], optimizationPathGD[1], 'ko')\n",
    "ax.plot(optimizationPathAdam[0], optimizationPathAdam[1], \"r\", label=\"Adam\")\n",
    "ax.plot(optimizationPathAdam[0], optimizationPathAdam[1], \"rs\")\n",
    "ax.plot([1], [1], \"bs\", markersize=12, label=\"minimum\")\n",
    "\n",
    "fig.colorbar(cp)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
